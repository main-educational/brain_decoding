
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Brain decoding with GCN &#8212; Introduction to brain decodin in fMRI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script async="async" kind="hypothesis" src="https://hypothes.is/embed.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gcn_decoding';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="canonical" href="https://main-educational.github.io/brain_encoding_decoding/gcn_decoding.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Brain encoding" href="encoding.html" />
    <link rel="prev" title="Brain decoding with MLP" href="mlp_decoding.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/neurolibre-logo.png" class="logo__image only-light" alt="Introduction to brain decodin in fMRI - Home"/>
    <script>document.write(`<img src="_static/neurolibre-logo.png" class="logo__image only-dark" alt="Introduction to brain decodin in fMRI - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="haxby_data.html">An overview of the Haxby dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="svm_decoding.html">Brain decoding with SVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlp_decoding.html">Brain decoding with MLP</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Brain decoding with GCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="encoding.html">Brain encoding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/main-educational/brain_encoding_decoding/main?urlpath=tree/content/gcn_decoding.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/main-educational/brain_encoding_decoding" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/main-educational/brain_encoding_decoding/edit/main/content/gcn_decoding.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/main-educational/brain_encoding_decoding/issues/new?title=Issue%20on%20page%20%2Fgcn_decoding.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/gcn_decoding.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Brain decoding with GCN</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#brain-graph-representation">Brain graph representation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolution-network-gcn">Graph Convolution Network (GCN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-brain-graph-for-gcn">Create brain graph for GCN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-dataset-for-model-training">Preparing the dataset for model training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-a-gcn-model">Generating a GCN model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-evaluating-the-model">Train and evaluating the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="brain-decoding-with-gcn">
<h1>Brain decoding with GCN<a class="headerlink" href="#brain-decoding-with-gcn" title="Link to this heading">#</a></h1>
<section id="brain-graph-representation">
<h2>Brain graph representation<a class="headerlink" href="#brain-graph-representation" title="Link to this heading">#</a></h2>
<p>Graph signal processing is a new tool to model brain organization and function. The brain is composed of several Region of Interests(ROIs). Brain graphs provide an efficient way for modeling the human brain connectome, by associating nodes to the brain regions, and defining edges via anatomical or functional connections. These ROIs are connected to some regions of interests with the highest connectivity.
<br/><br/></p>
<a class="reference internal image-reference" href="_images/Brain_connectivity_graph.png"><img alt="_images/Brain_connectivity_graph.png" src="_images/Brain_connectivity_graph.png" style="width: 545px; height: 194px;" /></a>
<p>Representation of Brain connectivity by graph theory.
Image source:<a class="reference external" href="https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-1/">https://atcold.github.io/pytorch-Deep-Learning/en/week13/13-1/</a></p>
</section>
<section id="graph-convolution-network-gcn">
<h2>Graph Convolution Network (GCN)<a class="headerlink" href="#graph-convolution-network-gcn" title="Link to this heading">#</a></h2>
<p><br/><br/>
<a class="reference internal" href="_images/GCN_pipeline_main2022.png"><img alt="_images/GCN_pipeline_main2022.png" src="_images/GCN_pipeline_main2022.png" style="width: 680px; height: 335px;" /></a></p>
<p>Schematic view of brain decoding using graph convolution network. Model is adapted from Zhang and colleagues (2021).
<strong>a)</strong> Bold time series are used to construct the brain graph by associating nodes to predefined brain regions (parcels) and indicating edges between each pair of brain regions based on the strength of their connections. Then, both brain graph and time-series matrix are imported into the graph convolutional network
<strong>b)</strong> The decoding model consists of three graph convolutional layers with 32 ChebNet graph filters at each layer,  followed by a global average pooling layer, two fully connected layers (MLP, consisting of 256-128 units) and softmax function. This pipeline generates task-specific representations of recorded brain activities and predicts the corresponding cognitive states.</p>
</section>
<section id="getting-the-data">
<h2>Getting the data<a class="headerlink" href="#getting-the-data" title="Link to this heading">#</a></h2>
<p>We are going to download the dataset from Haxby and colleagues (2001) <span id="id1">[<a class="reference internal" href="haxby_data.html#id6" title="J V Haxby, M I Gobbini, M L Furey, A Ishai, J L Schouten, and P Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293(5539):2425–2430, September 2001.">HGF+01</a>]</span>. You can check <a class="reference internal" href="haxby_data.html#haxby-dataset"><span class="std std-ref">An overview of the Haxby dataset</span></a> section for more details on that dataset. Here we are going to quickly download it, and prepare it for machine learning applications with a set of predictive variable, the brain time series, and a dependent variable, the annotation on cognition.</p>
<div class="cell tag_hide_input tag_hide_output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;once&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">nilearn.input_data</span> <span class="kn">import</span> <span class="n">NiftiMasker</span>

<span class="kn">from</span> <span class="nn">nilearn</span> <span class="kn">import</span> <span class="n">datasets</span>

<span class="c1"># We are fetching the data for subject 4</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s1">&#39;..&#39;</span><span class="p">,</span> <span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">sub_no</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">haxby_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">fetch_haxby</span><span class="p">(</span><span class="n">subjects</span><span class="o">=</span><span class="p">[</span><span class="n">sub_no</span><span class="p">],</span> <span class="n">fetch_stimuli</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">)</span>
<span class="n">func_file</span> <span class="o">=</span> <span class="n">haxby_dataset</span><span class="o">.</span><span class="n">func</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="c1"># Standardizing</span>
<span class="n">mask_vt_file</span> <span class="o">=</span> <span class="n">haxby_dataset</span><span class="o">.</span><span class="n">mask_vt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">masker</span> <span class="o">=</span> <span class="n">NiftiMasker</span><span class="p">(</span><span class="n">mask_img</span><span class="o">=</span><span class="n">mask_vt_file</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># cognitive annotations</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">behavioral</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">haxby_dataset</span><span class="o">.</span><span class="n">session_target</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39; &#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">masker</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">func_file</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">behavioral</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/nilearn/input_data/__init__.py:23: DeprecationWarning: The import path &#39;nilearn.input_data&#39; is deprecated in version 0.9. Importing from &#39;nilearn.input_data&#39; will be possible at least until release 0.13.0. Please import from &#39;nilearn.maskers&#39; instead.
  warnings.warn(message, DeprecationWarning)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Added README.md to ../data


Dataset created in ../data/haxby2001

Downloading data from https://www.nitrc.org/frs/download.php/7868/mask.nii.gz ...
Downloading data from http://data.pymvpa.org/datasets/haxby2001/MD5SUMS ...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> ...done. (0 seconds, 0 min)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> ...done. (0 seconds, 0 min)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data from http://data.pymvpa.org/datasets/haxby2001/subj4-2010.01.14.tar.gz ...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloaded 71909376 of 329954386 bytes (21.8%,    3.7s remaining)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloaded 166133760 of 329954386 bytes (50.4%,    2.0s remaining)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloaded 260833280 of 329954386 bytes (79.1%,    0.8s remaining)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> ...done. (4 seconds, 0 min)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Extracting data from ../data/haxby2001/622d4f5d4b8f14a567901606c924e90d/subj4-2010.01.14.tar.gz...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>.. done.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Downloading data from http://data.pymvpa.org/datasets/haxby2001/stimuli-2010.01.14.tar.gz ...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> ...done. (0 seconds, 0 min)
Extracting data from ../data/haxby2001/5cd78c74b711572c7f41a5bddb69abca/stimuli-2010.01.14.tar.gz..... done.
/opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/nilearn/image/resampling.py:492: UserWarning: The provided image has no sform in its header. Please check the provided file. Results may not be as expected.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/joblib/memory.py:312: DeprecationWarning: The default strategy for standardize is currently &#39;zscore&#39; which incorrectly uses population std to calculate sample zscores. The new strategy &#39;zscore_sample&#39; corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the &#39;zscore&#39; option will be removed. Please use &#39;zscore_sample&#39; instead.
  return self.func(*args, **kwargs)
</pre></div>
</div>
</div>
</div>
<p>Let’s check the shape of X and y and the cognitive annotations of this data sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">categories</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">categories</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;y:&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X:&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;rest&#39; &#39;face&#39; &#39;chair&#39; &#39;scissors&#39; &#39;shoe&#39; &#39;scrambledpix&#39; &#39;house&#39; &#39;cat&#39;
 &#39;bottle&#39;]
y: (1452,)
X: (1452, 675)
</pre></div>
</div>
</div>
</div>
<p>So we have 1452 time points in the imaging data, and for each time point we have recordings of fMRI activity across 675 brain regions.</p>
</section>
<section id="create-brain-graph-for-gcn">
<h2>Create brain graph for GCN<a class="headerlink" href="#create-brain-graph-for-gcn" title="Link to this heading">#</a></h2>
<p>A key component of GCN is brain graph.
Brain graph provides a network representation of brain organization by associating nodes to brain regions and defining edges via anatomical or functional connections.
After generating time series, we will firstly use the nilearn function to geneate a correlation based functional connectome.</p>
<div class="tip admonition">
<p class="admonition-title">Basic of graph laplacian and graph convolutional networks.</p>
<p>To explore the basics of <code class="docutils literal notranslate"><span class="pre">graph</span> <span class="pre">laplacian</span></code> and <code class="docutils literal notranslate"><span class="pre">graph</span> <span class="pre">convolutional</span> <span class="pre">networks</span></code> and how to apply these tools to neuroimging data check the tutorial from MAIN 2019 conference presented by Dr. Zhang.</p>
<p><a class="reference external" href="https://drive.google.com/file/d/1Gu28WcHXlwjXQSSmqZZwIcESHff_j-J4/view?usp=sharing">GCN_tutorial_slides:</a><br />
<a class="reference external" href="https://github.com/zhangyu2ustc/gcn_tutorial_test.git">Github repo:</a>
<a class="reference external" href="https://mybinder.org/v2/gh/zhangyu2ustc/gcn_tutorial_test/master?filepath=notebooks%2F">Binder projects:</a></p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s1">&#39;once&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">nilearn.connectome</span>

<span class="c1"># Estimating connectomes and save for pytorch to load</span>
<span class="n">corr_measure</span> <span class="o">=</span> <span class="n">nilearn</span><span class="o">.</span><span class="n">connectome</span><span class="o">.</span><span class="n">ConnectivityMeasure</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;correlation&quot;</span><span class="p">)</span>
<span class="n">conn</span> <span class="o">=</span> <span class="n">corr_measure</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([</span><span class="n">X</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">n_regions_extracted</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">title</span> <span class="o">=</span> <span class="s1">&#39;Correlation between </span><span class="si">%d</span><span class="s1"> regions&#39;</span> <span class="o">%</span> <span class="n">n_regions_extracted</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Correlation matrix shape:&#39;</span><span class="p">,</span><span class="n">conn</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># First plot the matrix</span>
<span class="kn">from</span> <span class="nn">nilearn</span> <span class="kn">import</span> <span class="n">plotting</span>
<span class="n">display</span> <span class="o">=</span> <span class="n">plotting</span><span class="o">.</span><span class="n">plot_matrix</span><span class="p">(</span><span class="n">conn</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">colorbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/opt/hostedtoolcache/Python/3.11.10/x64/lib/python3.11/site-packages/nilearn/connectome/connectivity_matrices.py:507: DeprecationWarning: The default strategy for standardize is currently &#39;zscore&#39; which incorrectly uses population std to calculate sample zscores. The new strategy &#39;zscore_sample&#39; corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the &#39;zscore&#39; option will be removed. Please use &#39;zscore_sample&#39; instead.
  covariances_std = [
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Correlation matrix shape: (675, 675)
</pre></div>
</div>
<img alt="_images/736b82156e36a0a3652f7ab309ce03b84f9d019a25fd60d88434fad30b1daf68.png" src="_images/736b82156e36a0a3652f7ab309ce03b84f9d019a25fd60d88434fad30b1daf68.png" />
</div>
</div>
<p>The next step is to construct the brain graph for GCN.</p>
<p><strong>k-Nearest Neighbours(KNN) graph</strong> for the group average connectome will be built based on the connectivity-matrix.</p>
<p>Each node is only connected to <em>k</em> conn = corr_measure.fit_transform([X])[0]
other neighbouring nodes.
For the purpose of demostration, we constrain the graph to from clusters with <strong>8</strong> neighbouring nodes with the strongest connectivity.</p>
<p>For more details you please check out <strong><em>src/graph_construction.py</em></strong> script.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../src&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">graph_construction</span> <span class="kn">import</span> <span class="n">make_group_graph</span>

<span class="c1"># make a graph for the subject</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">make_group_graph</span><span class="p">([</span><span class="n">conn</span><span class="p">],</span> <span class="n">self_loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">line</span> <span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">sys</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s1">&#39;../src&#39;</span><span class="p">)</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="kn">from</span> <span class="nn">graph_construction</span> <span class="kn">import</span> <span class="n">make_group_graph</span>
<span class="g g-Whitespace">      </span><span class="mi">5</span> <span class="c1"># make a graph for the subject</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="n">graph</span> <span class="o">=</span> <span class="n">make_group_graph</span><span class="p">([</span><span class="n">conn</span><span class="p">],</span> <span class="n">self_loops</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">work</span><span class="o">/</span><span class="n">brain_encoding_decoding</span><span class="o">/</span><span class="n">brain_encoding_decoding</span><span class="o">/</span><span class="n">content</span><span class="o">/../</span><span class="n">src</span><span class="o">/</span><span class="n">graph_construction</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">3</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">torch</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="kn">import</span> <span class="nn">torch_geometric</span> <span class="k">as</span> <span class="nn">tg</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">def</span> <span class="nf">_make_undirected</span><span class="p">(</span><span class="n">mat</span><span class="p">):</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span><span class="w">     </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span><span class="sd">     Takes an input adjacency matrix and makes it undirected (symmetric).</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span><span class="sd"> </span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span><span class="sd">         Square adjacency matrix.</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span><span class="sd">     &quot;&quot;&quot;</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;torch_geometric&#39;
</pre></div>
</div>
</div>
</div>
</section>
<section id="preparing-the-dataset-for-model-training">
<h2>Preparing the dataset for model training<a class="headerlink" href="#preparing-the-dataset-for-model-training" title="Link to this heading">#</a></h2>
<p>The trials for different object categories are scattered in the experiment.
Firstly we will concatenated the volumes of the same category together.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># cancatenate the same type of trials</span>
<span class="n">concat_bold</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">categories</span><span class="p">:</span>
    <span class="n">cur_label_index</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">index</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">label</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="n">curr_bold_seg</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">cur_label_index</span><span class="p">]</span>    
    <span class="n">concat_bold</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">curr_bold_seg</span>
</pre></div>
</div>
</div>
</div>
<p>We split the data by the time window size that we wish to use to caputre the temporal dynamic.
Different lengths for our input data can be selected.
In this example we will continue with <strong><em>window_length = 1</em></strong>, which means each input file will have a length equal to just one Repetition Time (TR).
The splitted timeseries are saved as individual files (in the format of <code class="docutils literal notranslate"><span class="pre">&lt;category&gt;_seg_&lt;serialnumber&gt;.npy</span></code>),
the file names and the associated label are stored in the same directory,
under a file named <code class="docutils literal notranslate"><span class="pre">label.csv</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split the data by time window size and save to file</span>
<span class="n">window_length</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">dic_labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">name</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">categories</span><span class="p">)}</span>

<span class="c1"># set output paths</span>
<span class="n">split_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;haxby_split_win/&#39;</span><span class="p">)</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">split_path</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">split_path</span><span class="p">)</span>
<span class="n">out_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">split_path</span><span class="p">,</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1">_</span><span class="si">{:04d}</span><span class="s1">.npy&#39;</span><span class="p">)</span>
<span class="n">out_csv</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">split_path</span><span class="p">,</span> <span class="s1">&#39;labels.csv&#39;</span><span class="p">)</span>

<span class="n">label_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;filename&#39;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">ts_data</span> <span class="ow">in</span> <span class="n">concat_bold</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">ts_duration</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ts_data</span><span class="p">)</span>
    <span class="n">ts_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">_seg&quot;</span>
    <span class="n">valid_label</span> <span class="o">=</span> <span class="n">dic_labels</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>

    <span class="c1"># Split the timeseries</span>
    <span class="n">rem</span> <span class="o">=</span> <span class="n">ts_duration</span> <span class="o">%</span> <span class="n">window_length</span>
    <span class="n">n_splits</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">ts_duration</span> <span class="o">/</span> <span class="n">window_length</span><span class="p">))</span>

    <span class="n">ts_data</span> <span class="o">=</span> <span class="n">ts_data</span><span class="p">[:(</span><span class="n">ts_duration</span> <span class="o">-</span> <span class="n">rem</span><span class="p">),</span> <span class="p">:]</span>   

    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">split_ts</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">ts_data</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">)):</span>
        <span class="n">ts_output_file_name</span> <span class="o">=</span> <span class="n">out_file</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ts_filename</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>

        <span class="n">split_ts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">split_ts</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">np</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">ts_output_file_name</span><span class="p">,</span> <span class="n">split_ts</span><span class="p">)</span>

        <span class="n">curr_label</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">valid_label</span><span class="p">,</span> <span class="s1">&#39;filename&#39;</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">ts_output_file_name</span><span class="p">)}</span>
        <span class="n">label_df</span> <span class="o">=</span> <span class="n">label_df</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">curr_label</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        
<span class="n">label_df</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">out_csv</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  
</pre></div>
</div>
</div>
</div>
<p>Now we use a customised <code class="docutils literal notranslate"><span class="pre">pytorch</span></code> dataset generator class <code class="docutils literal notranslate"><span class="pre">TimeWindowsDataset</span></code> to split the data into training,
validation, and testing sets for model selection.</p>
<p>The dataset generator defaults isolates 20% of the data as the validation set, and 10% as testing set.
For more details of customising a dataset, please see <code class="docutils literal notranslate"><span class="pre">src/gcn_windows_dataset.py</span></code> and the
official <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"><code class="docutils literal notranslate"><span class="pre">pytorch</span></code> documentation</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># split dataset</span>
<span class="kn">from</span> <span class="nn">gcn_windows_dataset</span> <span class="kn">import</span> <span class="n">TimeWindowsDataset</span>

<span class="n">random_seed</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TimeWindowsDataset</span><span class="p">(</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="n">split_path</span><span class="p">,</span> 
    <span class="n">partition</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> 
    <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span> 
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">TimeWindowsDataset</span><span class="p">(</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="n">split_path</span><span class="p">,</span> 
    <span class="n">partition</span><span class="o">=</span><span class="s2">&quot;valid&quot;</span><span class="p">,</span> 
    <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span> 
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">TimeWindowsDataset</span><span class="p">(</span>
    <span class="n">data_dir</span><span class="o">=</span><span class="n">split_path</span><span class="p">,</span> 
    <span class="n">partition</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">,</span> 
    <span class="n">random_seed</span><span class="o">=</span><span class="n">random_seed</span><span class="p">,</span> 
    <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train dataset: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;valid dataset: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test dataset: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train dataset: 1016*(torch.Size([675, 1]), ())
valid dataset: 290*(torch.Size([675, 1]), ())
test dataset: 146*(torch.Size([675, 1]), ())
</pre></div>
</div>
</div>
</div>
<p>Once the datasets are created, we can use the pytorch <a class="reference external" href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders">data loader</a> to iterate through the data during the model selection process.
The <strong>batch size</strong> defines the number of samples that will be propagated through the neural network.
We are separating the dataset into 10 time windows per batch.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">random_seed</span><span class="p">)</span>
<span class="n">train_generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_generator</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">train_features</span><span class="p">,</span> <span class="n">train_labels</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_generator</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Feature batch shape: </span><span class="si">{</span><span class="n">train_features</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">; mean </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">train_features</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Labels batch shape: </span><span class="si">{</span><span class="n">train_labels</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="si">}</span><span class="s2">; mean </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="o">.</span><span class="n">float</span><span class="p">(</span><span class="n">train_labels</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Feature batch shape: torch.Size([10, 675, 1]); mean -5.08626296635839e-09
Labels batch shape: torch.Size([10]); mean 2.5999999046325684
</pre></div>
</div>
</div>
</div>
</section>
<section id="generating-a-gcn-model">
<h2>Generating a GCN model<a class="headerlink" href="#generating-a-gcn-model" title="Link to this heading">#</a></h2>
<p>We have created a GCN of the following property:</p>
<ul class="simple">
<li><p><strong>3</strong> graph convolutional layers</p></li>
<li><p><strong>32 graph filters</strong>  at each layer</p></li>
<li><p>followed by a <strong>global average pooling</strong> layer</p></li>
<li><p><strong>2 fully connected</strong> layers</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gcn_model</span> <span class="kn">import</span> <span class="n">GCN</span>

<span class="n">gcn</span> <span class="o">=</span> <span class="n">GCN</span><span class="p">(</span><span class="n">graph</span><span class="o">.</span><span class="n">edge_index</span><span class="p">,</span> 
          <span class="n">graph</span><span class="o">.</span><span class="n">edge_attr</span><span class="p">,</span> 
          <span class="n">n_roi</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
          <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
          <span class="n">n_timepoints</span><span class="o">=</span><span class="n">window_length</span><span class="p">,</span> 
          <span class="n">n_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">categories</span><span class="p">))</span>
<span class="n">gcn</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GCN(
  (conv1): ChebConv(1, 32, K=2, normalization=sym)
  (conv2): ChebConv(32, 32, K=2, normalization=sym)
  (conv3): ChebConv(32, 10, K=2, normalization=sym)
  (fc1): Linear(in_features=6750, out_features=256, bias=True)
  (fc2): Linear(in_features=256, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=9, bias=True)
  (dropout): Dropout(p=0.2, inplace=False)
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-and-evaluating-the-model">
<h2>Train and evaluating the model<a class="headerlink" href="#train-and-evaluating-the-model" title="Link to this heading">#</a></h2>
<p>We will use a procedure called backpropagation to train the model.
When we training the model with the first batch of data, the accuarcy and loss will be pretty poor.
Backpropagation is an algorithm to update the model based on the rate of loss.
Iterating through each batch, the model will be updated and reduce the loss.</p>
<p>Function <code class="docutils literal notranslate"><span class="pre">training_loop</span></code> performs backpropagation through pytorch.
One can use their own choice of optimizer for backpropagation and estimator for loss.</p>
<p>After one round of training, we use the validation dataset to calculate the average accuracy and loss with function <code class="docutils literal notranslate"><span class="pre">valid_test_loop</span></code>.
These metrics will serve as the reference for model performance of this round of training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>    

    <span class="k">for</span> <span class="n">batch</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloader</span><span class="p">):</span>
        <span class="c1"># Compute prediction and loss</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

        <span class="c1"># Backpropagation</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">loss</span><span class="p">,</span> <span class="n">current</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">batch</span> <span class="o">*</span> <span class="n">dataloader</span><span class="o">.</span><span class="n">batch_size</span>

        <span class="n">correct</span> <span class="o">=</span> <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">correct</span> <span class="o">/=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">batch</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">current</span> <span class="o">==</span> <span class="n">size</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;#</span><span class="si">{</span><span class="n">batch</span><span class="si">:</span><span class="s2">&gt;5</span><span class="si">}</span><span class="s2">;</span><span class="se">\t</span><span class="s2">train_loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">&gt;0.3f</span><span class="si">}</span><span class="s2">;</span><span class="se">\t</span><span class="s2">train_accuracy:</span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;5.1f</span><span class="si">}</span><span class="s2">%</span><span class="se">\t\t</span><span class="s2">[</span><span class="si">{</span><span class="n">current</span><span class="si">:</span><span class="s2">&gt;5d</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">size</span><span class="si">:</span><span class="s2">&gt;5d</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>

        
<span class="k">def</span> <span class="nf">valid_test_loop</span><span class="p">(</span><span class="n">dataloader</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">):</span>
    <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">:</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">+=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">pred</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

    <span class="n">loss</span> <span class="o">/=</span> <span class="n">size</span>
    <span class="n">correct</span> <span class="o">/=</span> <span class="n">size</span>

    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span>
</pre></div>
</div>
</div>
</div>
<p>This whole procedure described above is called an <strong>epoch</strong>.
We will repeat the process for 25 epochs.
Here the choice of loss function is <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> and the optimizer to update the model is <code class="docutils literal notranslate"><span class="pre">Adam</span></code>.</p>
<div class="cell tag_hide_output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">gcn</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">25</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">epochs</span><span class="si">}</span><span class="se">\n</span><span class="s2">-------------------------------&quot;</span><span class="p">)</span>
    <span class="n">train_loop</span><span class="p">(</span><span class="n">train_generator</span><span class="p">,</span> <span class="n">gcn</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="n">valid_test_loop</span><span class="p">(</span><span class="n">valid_generator</span><span class="p">,</span> <span class="n">gcn</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Valid metrics:</span><span class="se">\n\t</span><span class="s2"> avg_loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">&gt;8f</span><span class="si">}</span><span class="s2">;</span><span class="se">\t</span><span class="s2"> avg_accuracy: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/25
-------------------------------
#    0;	train_loss: 2.210;	train_accuracy:  0.0%		[    0/ 1016]
#   10;	train_loss: 1.862;	train_accuracy: 40.0%		[  100/ 1016]
#   20;	train_loss: 1.909;	train_accuracy: 40.0%		[  200/ 1016]
#   30;	train_loss: 1.880;	train_accuracy: 40.0%		[  300/ 1016]
#   40;	train_loss: 2.055;	train_accuracy: 30.0%		[  400/ 1016]
#   50;	train_loss: 1.545;	train_accuracy: 60.0%		[  500/ 1016]
#   60;	train_loss: 1.644;	train_accuracy: 50.0%		[  600/ 1016]
#   70;	train_loss: 1.511;	train_accuracy: 60.0%		[  700/ 1016]
#   80;	train_loss: 2.151;	train_accuracy: 30.0%		[  800/ 1016]
#   90;	train_loss: 1.991;	train_accuracy: 30.0%		[  900/ 1016]
#  100;	train_loss: 1.822;	train_accuracy: 40.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.177827;	 avg_accuracy: 43.4%
Epoch 2/25
-------------------------------
#    0;	train_loss: 1.815;	train_accuracy: 30.0%		[    0/ 1016]
#   10;	train_loss: 1.953;	train_accuracy: 30.0%		[  100/ 1016]
#   20;	train_loss: 1.549;	train_accuracy: 40.0%		[  200/ 1016]
#   30;	train_loss: 2.179;	train_accuracy: 20.0%		[  300/ 1016]
#   40;	train_loss: 2.002;	train_accuracy: 40.0%		[  400/ 1016]
#   50;	train_loss: 1.966;	train_accuracy: 20.0%		[  500/ 1016]
#   60;	train_loss: 1.559;	train_accuracy: 60.0%		[  600/ 1016]
#   70;	train_loss: 1.871;	train_accuracy: 40.0%		[  700/ 1016]
#   80;	train_loss: 2.170;	train_accuracy: 30.0%		[  800/ 1016]
#   90;	train_loss: 1.548;	train_accuracy: 60.0%		[  900/ 1016]
#  100;	train_loss: 1.757;	train_accuracy: 50.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.155729;	 avg_accuracy: 48.6%
Epoch 3/25
-------------------------------
#    0;	train_loss: 1.833;	train_accuracy: 30.0%		[    0/ 1016]
#   10;	train_loss: 1.531;	train_accuracy: 60.0%		[  100/ 1016]
#   20;	train_loss: 1.439;	train_accuracy: 50.0%		[  200/ 1016]
#   30;	train_loss: 1.271;	train_accuracy: 60.0%		[  300/ 1016]
#   40;	train_loss: 1.097;	train_accuracy: 70.0%		[  400/ 1016]
#   50;	train_loss: 1.350;	train_accuracy: 50.0%		[  500/ 1016]
#   60;	train_loss: 1.045;	train_accuracy: 60.0%		[  600/ 1016]
#   70;	train_loss: 2.200;	train_accuracy: 20.0%		[  700/ 1016]
#   80;	train_loss: 1.559;	train_accuracy: 40.0%		[  800/ 1016]
#   90;	train_loss: 1.638;	train_accuracy: 60.0%		[  900/ 1016]
#  100;	train_loss: 1.644;	train_accuracy: 50.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.134931;	 avg_accuracy: 56.9%
Epoch 4/25
-------------------------------
#    0;	train_loss: 1.351;	train_accuracy: 70.0%		[    0/ 1016]
#   10;	train_loss: 1.477;	train_accuracy: 40.0%		[  100/ 1016]
#   20;	train_loss: 1.278;	train_accuracy: 70.0%		[  200/ 1016]
#   30;	train_loss: 0.708;	train_accuracy: 80.0%		[  300/ 1016]
#   40;	train_loss: 0.855;	train_accuracy: 80.0%		[  400/ 1016]
#   50;	train_loss: 1.362;	train_accuracy: 60.0%		[  500/ 1016]
#   60;	train_loss: 1.138;	train_accuracy: 60.0%		[  600/ 1016]
#   70;	train_loss: 1.040;	train_accuracy: 70.0%		[  700/ 1016]
#   80;	train_loss: 1.974;	train_accuracy: 50.0%		[  800/ 1016]
#   90;	train_loss: 0.838;	train_accuracy: 70.0%		[  900/ 1016]
#  100;	train_loss: 1.547;	train_accuracy: 30.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.123752;	 avg_accuracy: 61.0%
Epoch 5/25
-------------------------------
#    0;	train_loss: 1.023;	train_accuracy: 80.0%		[    0/ 1016]
#   10;	train_loss: 0.753;	train_accuracy: 80.0%		[  100/ 1016]
#   20;	train_loss: 1.136;	train_accuracy: 60.0%		[  200/ 1016]
#   30;	train_loss: 1.043;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 1.439;	train_accuracy: 60.0%		[  400/ 1016]
#   50;	train_loss: 1.184;	train_accuracy: 50.0%		[  500/ 1016]
#   60;	train_loss: 0.730;	train_accuracy: 80.0%		[  600/ 1016]
#   70;	train_loss: 0.860;	train_accuracy: 70.0%		[  700/ 1016]
#   80;	train_loss: 0.882;	train_accuracy: 70.0%		[  800/ 1016]
#   90;	train_loss: 1.734;	train_accuracy: 60.0%		[  900/ 1016]
#  100;	train_loss: 1.106;	train_accuracy: 70.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.106471;	 avg_accuracy: 65.5%
Epoch 6/25
-------------------------------
#    0;	train_loss: 0.945;	train_accuracy: 70.0%		[    0/ 1016]
#   10;	train_loss: 0.731;	train_accuracy: 70.0%		[  100/ 1016]
#   20;	train_loss: 1.188;	train_accuracy: 70.0%		[  200/ 1016]
#   30;	train_loss: 0.693;	train_accuracy: 80.0%		[  300/ 1016]
#   40;	train_loss: 0.622;	train_accuracy: 80.0%		[  400/ 1016]
#   50;	train_loss: 0.876;	train_accuracy: 80.0%		[  500/ 1016]
#   60;	train_loss: 0.375;	train_accuracy: 90.0%		[  600/ 1016]
#   70;	train_loss: 0.844;	train_accuracy: 80.0%		[  700/ 1016]
#   80;	train_loss: 0.963;	train_accuracy: 60.0%		[  800/ 1016]
#   90;	train_loss: 1.578;	train_accuracy: 50.0%		[  900/ 1016]
#  100;	train_loss: 0.627;	train_accuracy: 80.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.100941;	 avg_accuracy: 68.3%
Epoch 7/25
-------------------------------
#    0;	train_loss: 0.399;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.661;	train_accuracy: 90.0%		[  100/ 1016]
#   20;	train_loss: 0.555;	train_accuracy: 90.0%		[  200/ 1016]
#   30;	train_loss: 1.038;	train_accuracy: 60.0%		[  300/ 1016]
#   40;	train_loss: 0.352;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.803;	train_accuracy: 70.0%		[  500/ 1016]
#   60;	train_loss: 0.927;	train_accuracy: 80.0%		[  600/ 1016]
#   70;	train_loss: 0.510;	train_accuracy: 90.0%		[  700/ 1016]
#   80;	train_loss: 1.013;	train_accuracy: 70.0%		[  800/ 1016]
#   90;	train_loss: 0.560;	train_accuracy: 80.0%		[  900/ 1016]
#  100;	train_loss: 1.095;	train_accuracy: 60.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.086677;	 avg_accuracy: 73.1%
Epoch 8/25
-------------------------------
#    0;	train_loss: 0.818;	train_accuracy: 80.0%		[    0/ 1016]
#   10;	train_loss: 0.572;	train_accuracy: 80.0%		[  100/ 1016]
#   20;	train_loss: 0.549;	train_accuracy: 80.0%		[  200/ 1016]
#   30;	train_loss: 0.561;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.316;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.435;	train_accuracy: 90.0%		[  500/ 1016]
#   60;	train_loss: 1.031;	train_accuracy: 60.0%		[  600/ 1016]
#   70;	train_loss: 0.722;	train_accuracy: 70.0%		[  700/ 1016]
#   80;	train_loss: 0.558;	train_accuracy: 70.0%		[  800/ 1016]
#   90;	train_loss: 0.193;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.932;	train_accuracy: 60.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.082203;	 avg_accuracy: 71.7%
Epoch 9/25
-------------------------------
#    0;	train_loss: 0.449;	train_accuracy: 80.0%		[    0/ 1016]
#   10;	train_loss: 0.479;	train_accuracy: 80.0%		[  100/ 1016]
#   20;	train_loss: 0.448;	train_accuracy: 90.0%		[  200/ 1016]
#   30;	train_loss: 0.685;	train_accuracy: 60.0%		[  300/ 1016]
#   40;	train_loss: 0.447;	train_accuracy: 80.0%		[  400/ 1016]
#   50;	train_loss: 0.217;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.134;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.081;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 1.154;	train_accuracy: 70.0%		[  800/ 1016]
#   90;	train_loss: 0.714;	train_accuracy: 80.0%		[  900/ 1016]
#  100;	train_loss: 0.296;	train_accuracy: 90.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.083770;	 avg_accuracy: 70.7%
Epoch 10/25
-------------------------------
#    0;	train_loss: 0.335;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.295;	train_accuracy: 90.0%		[  100/ 1016]
#   20;	train_loss: 0.335;	train_accuracy: 90.0%		[  200/ 1016]
#   30;	train_loss: 0.280;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.297;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.039;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.135;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.281;	train_accuracy: 90.0%		[  700/ 1016]
#   80;	train_loss: 0.623;	train_accuracy: 80.0%		[  800/ 1016]
#   90;	train_loss: 0.335;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.297;	train_accuracy: 80.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.084373;	 avg_accuracy: 73.4%
Epoch 11/25
-------------------------------
#    0;	train_loss: 0.263;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.206;	train_accuracy: 90.0%		[  100/ 1016]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#   20;	train_loss: 0.601;	train_accuracy: 80.0%		[  200/ 1016]
#   30;	train_loss: 0.361;	train_accuracy: 80.0%		[  300/ 1016]
#   40;	train_loss: 0.391;	train_accuracy: 80.0%		[  400/ 1016]
#   50;	train_loss: 0.030;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.147;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.282;	train_accuracy: 90.0%		[  700/ 1016]
#   80;	train_loss: 0.069;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.374;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.243;	train_accuracy: 90.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.073709;	 avg_accuracy: 78.3%
Epoch 12/25
-------------------------------
#    0;	train_loss: 0.677;	train_accuracy: 80.0%		[    0/ 1016]
#   10;	train_loss: 0.326;	train_accuracy: 90.0%		[  100/ 1016]
#   20;	train_loss: 0.446;	train_accuracy: 80.0%		[  200/ 1016]
#   30;	train_loss: 0.211;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.191;	train_accuracy: 90.0%		[  400/ 1016]
#   50;	train_loss: 0.018;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.044;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.347;	train_accuracy: 80.0%		[  700/ 1016]
#   80;	train_loss: 0.027;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.146;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.136;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.074484;	 avg_accuracy: 79.3%
Epoch 13/25
-------------------------------
#    0;	train_loss: 0.134;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.100;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.245;	train_accuracy: 90.0%		[  200/ 1016]
#   30;	train_loss: 0.335;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.109;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.057;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.125;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.067;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.160;	train_accuracy: 90.0%		[  800/ 1016]
#   90;	train_loss: 0.062;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.240;	train_accuracy: 90.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.074189;	 avg_accuracy: 79.3%
Epoch 14/25
-------------------------------
#    0;	train_loss: 0.192;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.022;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.222;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.029;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.625;	train_accuracy: 80.0%		[  400/ 1016]
#   50;	train_loss: 0.223;	train_accuracy: 90.0%		[  500/ 1016]
#   60;	train_loss: 0.098;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.627;	train_accuracy: 90.0%		[  700/ 1016]
#   80;	train_loss: 0.063;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.284;	train_accuracy: 80.0%		[  900/ 1016]
#  100;	train_loss: 0.051;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.087015;	 avg_accuracy: 76.6%
Epoch 15/25
-------------------------------
#    0;	train_loss: 0.092;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.040;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.056;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.032;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.085;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.114;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.041;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.087;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.076;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.011;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.045;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.070456;	 avg_accuracy: 82.8%
Epoch 16/25
-------------------------------
#    0;	train_loss: 0.084;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.068;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.039;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.014;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.111;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.019;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.020;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.107;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.103;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.007;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.099;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.080272;	 avg_accuracy: 79.3%
Epoch 17/25
-------------------------------
#    0;	train_loss: 0.017;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.035;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.055;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.017;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.013;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.079;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.024;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.024;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.010;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.166;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.154;	train_accuracy: 90.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.076693;	 avg_accuracy: 80.0%
Epoch 18/25
-------------------------------
#    0;	train_loss: 0.020;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.079;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.012;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.054;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.054;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.025;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.028;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.051;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.054;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.190;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.020;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.083471;	 avg_accuracy: 82.4%
Epoch 19/25
-------------------------------
#    0;	train_loss: 0.039;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.028;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.006;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.247;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.136;	train_accuracy: 90.0%		[  400/ 1016]
#   50;	train_loss: 0.083;	train_accuracy: 90.0%		[  500/ 1016]
#   60;	train_loss: 0.037;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.061;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.016;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.072;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.004;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.088379;	 avg_accuracy: 78.6%
Epoch 20/25
-------------------------------
#    0;	train_loss: 0.044;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.057;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.034;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.032;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.267;	train_accuracy: 90.0%		[  400/ 1016]
#   50;	train_loss: 0.003;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.004;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.393;	train_accuracy: 80.0%		[  700/ 1016]
#   80;	train_loss: 0.010;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.167;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.011;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.096028;	 avg_accuracy: 78.6%
Epoch 21/25
-------------------------------
#    0;	train_loss: 0.384;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.002;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.004;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.282;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.009;	train_accuracy:100.0%		[  400/ 1016]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#   50;	train_loss: 0.073;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.098;	train_accuracy: 90.0%		[  600/ 1016]
#   70;	train_loss: 0.134;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.015;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.072;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.040;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.089697;	 avg_accuracy: 81.4%
Epoch 22/25
-------------------------------
#    0;	train_loss: 0.003;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.014;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.010;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.003;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.011;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.048;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.002;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.019;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.109;	train_accuracy: 90.0%		[  800/ 1016]
#   90;	train_loss: 0.083;	train_accuracy: 90.0%		[  900/ 1016]
#  100;	train_loss: 0.020;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.093843;	 avg_accuracy: 79.0%
Epoch 23/25
-------------------------------
#    0;	train_loss: 0.110;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.101;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.002;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.015;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.035;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.017;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.074;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.004;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.058;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.008;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.034;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.084977;	 avg_accuracy: 81.4%
Epoch 24/25
-------------------------------
#    0;	train_loss: 0.156;	train_accuracy: 90.0%		[    0/ 1016]
#   10;	train_loss: 0.004;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.002;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.002;	train_accuracy:100.0%		[  300/ 1016]
#   40;	train_loss: 0.019;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.050;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.005;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.008;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.036;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.028;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.004;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.097495;	 avg_accuracy: 79.7%
Epoch 25/25
-------------------------------
#    0;	train_loss: 0.012;	train_accuracy:100.0%		[    0/ 1016]
#   10;	train_loss: 0.005;	train_accuracy:100.0%		[  100/ 1016]
#   20;	train_loss: 0.126;	train_accuracy:100.0%		[  200/ 1016]
#   30;	train_loss: 0.101;	train_accuracy: 90.0%		[  300/ 1016]
#   40;	train_loss: 0.037;	train_accuracy:100.0%		[  400/ 1016]
#   50;	train_loss: 0.032;	train_accuracy:100.0%		[  500/ 1016]
#   60;	train_loss: 0.013;	train_accuracy:100.0%		[  600/ 1016]
#   70;	train_loss: 0.019;	train_accuracy:100.0%		[  700/ 1016]
#   80;	train_loss: 0.002;	train_accuracy:100.0%		[  800/ 1016]
#   90;	train_loss: 0.003;	train_accuracy:100.0%		[  900/ 1016]
#  100;	train_loss: 0.001;	train_accuracy:100.0%		[ 1000/ 1016]
Valid metrics:
	 avg_loss: 0.098641;	 avg_accuracy: 79.7%
</pre></div>
</div>
</div>
</div>
<p>After training the model for 25 epochs, we use the untouched test data to evaluate the model and conclude the results of training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># results</span>
<span class="n">loss</span><span class="p">,</span> <span class="n">correct</span> <span class="o">=</span> <span class="n">valid_test_loop</span><span class="p">(</span><span class="n">test_generator</span><span class="p">,</span> <span class="n">gcn</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test metrics:</span><span class="se">\n\t</span><span class="s2"> avg_loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">&gt;f</span><span class="si">}</span><span class="s2">;</span><span class="se">\t</span><span class="s2"> avg_accuracy: </span><span class="si">{</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">correct</span><span class="p">)</span><span class="si">:</span><span class="s2">&gt;0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test metrics:
	 avg_loss: 0.103537;	 avg_accuracy: 77.4%
</pre></div>
</div>
</div>
</div>
<p>The performance is good but how could we still improve it?</p>
</section>
<section id="exercises">
<h2>Exercises<a class="headerlink" href="#exercises" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Try out different time window sizes, batch size for the dataset,</p></li>
<li><p>Try different brain graph construction methods.</p></li>
<li><p>Try use different loss function or optimizer function.</p></li>
<li><p><strong>Hard</strong>: Treat the parameters you changed, such as time window size and batch size, as parameters of part of the model training.</p></li>
<li><p><strong>Hard</strong>: Try extracting regions from network components using dictionary learning for estimating brain networks.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id2">
<div role="list" class="citation-list">
<div class="citation" id="id4" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">HGF+01</a><span class="fn-bracket">]</span></span>
<p>J V Haxby, M I Gobbini, M L Furey, A Ishai, J L Schouten, and P Pietrini. Distributed and overlapping representations of faces and objects in ventral temporal cortex. <em>Science</em>, 293(5539):2425–2430, September 2001.</p>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="mlp_decoding.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Brain decoding with MLP</p>
      </div>
    </a>
    <a class="right-next"
       href="encoding.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Brain encoding</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#brain-graph-representation">Brain graph representation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#graph-convolution-network-gcn">Graph Convolution Network (GCN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#getting-the-data">Getting the data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#create-brain-graph-for-gcn">Create brain graph for GCN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparing-the-dataset-for-model-training">Preparing the dataset for model training</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-a-gcn-model">Generating a GCN model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-and-evaluating-the-model">Train and evaluating the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exercises">Exercises</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The MAIN Educational Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>