{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "251058d3",
   "metadata": {},
   "source": [
    "# Brain decoding with GCN\n",
    "\n",
    "## Graph Convolution Network (GCN)\n",
    "```{figure} gcn_decoding/GCN_pipeline.png\n",
    "---\n",
    "width: 500px\n",
    "name: gcn-pipeline-fig\n",
    "---\n",
    "Schematic of the analysis proposed in Zhang and colleagues (2021).\n",
    "The full time series are used to constrcut the brain graph to a network representation of brain organization by associating nodes to brain regions and defining edges via functional connections. \n",
    "\n",
    "```\n",
    "\n",
    "## Getting the data\n",
    "\n",
    "We are going to download the dataset from Haxby and colleagues (2001) {cite:p}`Haxby2001-vt`. You can check section {ref}`haxby-dataset` for more details on that dataset. Here we are going to quickly download it, and prepare it for machine learning applications with a set of predictive variable, the brain time series, and a dependent variable, the annotation on cognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186fca5",
   "metadata": {
    "tags": [
     "hide_input",
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "from nilearn import datasets\n",
    "# We are fetching the data for subject 4\n",
    "data_dir = os.path.join('..', 'data')\n",
    "sub_no = 4\n",
    "haxby_dataset = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)\n",
    "func_file = haxby_dataset.func[0]\n",
    "\n",
    "# cognitive annotations\n",
    "import pandas as pd\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "y = behavioral['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8e803",
   "metadata": {},
   "source": [
    "Let's check the size of dependent variable `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dff70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = y.unique()\n",
    "print(categories)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40132289",
   "metadata": {},
   "source": [
    "The generation of brain time series is a little bit more complicated for the GCN framework.\n",
    "The GCN framework from Zhang and colleagues (2021) {cite:p}`Zhang2021-fa` require a full brain graph. \n",
    "\n",
    "## Extract time series from a full brain atlas\n",
    "\n",
    "There are two common approaches to define the brain regions: using predefined atlases from published studies or generate from own data.\n",
    "As the Haxby dataset is shipped in the native resolution, we cannot easily use an published atlas.  \n",
    "Here we will demostrate how to use nilearn to generate the brain regions, extract signals, and calculate the brain graph.\n",
    "\n",
    "### Dictionary learning for estimating brain networks\n",
    "\n",
    "Nilearn provides several methods for data-driven brain network estimation and dictionary learning is one of the robust method. \n",
    "Dictionary learning (or sparse coding) is a representation learning method aiming at finding a sparse representation of the input data as a linear combination of basic elements called atoms. \n",
    "The identification of these atoms composing the dictionary relies on a sparsity principle: \n",
    "maximally sparse representations of the dataset are sought for. Atoms are not required to be orthogonal.\n",
    "\n",
    "We use the nilearn function `DictLearning` to estimate networks on the haxby EPI data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d49f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "from nilearn.decomposition import DictLearning\n",
    "\n",
    "# Initialize DictLearning object\n",
    "dict_learn = DictLearning(n_components=20, smoothing_fwhm=6.,\n",
    "                          low_pass=0.1, high_pass=0.01, t_r=2,\n",
    "                          detrend=True, standardize=True,\n",
    "                          memory=\"nilearn_cache\", memory_level=2,\n",
    "                          random_state=0)\n",
    "# Fit to the data\n",
    "dict_learn.fit(func_file)\n",
    "# Resting state networks/maps in attribute `components_img_`\n",
    "components_img = dict_learn.components_img_\n",
    "\n",
    "# Visualization of functional networks\n",
    "# Show networks using plotting utilities\n",
    "from nilearn.image import mean_img\n",
    "from nilearn import plotting\n",
    "mean_haxby = mean_img(func_file)\n",
    "plotting.plot_prob_atlas(components_img, \n",
    "                         bg_img=mean_haxby, \n",
    "                         view_type='filled_contours',\n",
    "                         title='Dictionary Learning maps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40791e9c",
   "metadata": {},
   "source": [
    "### Region extraction from network components\n",
    "\n",
    "This approach has been previously used in the neuroscience literature to study the intrinsic organization of brain anatomy and functions.\n",
    "The next step is to separate the learned networks into discrete regions.\n",
    "Nilearn provides an useful class `RegionExtractor` to extract isolated regions from statistical maps.\n",
    "As the networks generated from dictionary learning are denoted by probablilty rather than discrete values,\n",
    "we will use `RegionExtractor` to generate a parcellation scheme.\n",
    "\n",
    "```{admonition} Extract connected regions from a brain atlas image defined by labels (integers). \n",
    ":class: tip\n",
    "See function `nilearn.regions.connected_label_regions` and the tutorial on \n",
    "[Yeo 7 networks](https://nilearn.github.io/auto_examples/06_manipulating_images/plot_extract_regions_labels_image.html#sphx-glr-auto-examples-06-manipulating-images-plot-extract-regions-labels-image-py)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df64896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "from nilearn.regions import RegionExtractor\n",
    "\n",
    "extractor = RegionExtractor(components_img, threshold=0.5,\n",
    "                            thresholding_strategy='ratio_n_voxels',\n",
    "                            extractor='local_regions',\n",
    "                            low_pass=0.1, high_pass=0.01, t_r=2,\n",
    "                            detrend=True, standardize=True)\n",
    "# Just call fit() to process for regions extraction\n",
    "extractor.fit()\n",
    "# Extracted regions are stored in regions_img_\n",
    "regions_extracted_img = extractor.regions_img_\n",
    "# Each region index is stored in index_\n",
    "regions_index = extractor.index_\n",
    "# Total number of regions extracted\n",
    "n_regions_extracted = regions_extracted_img.shape[-1]\n",
    "\n",
    "# Visualization of region extraction results\n",
    "title = ('%d regions are extracted from %d components.'\n",
    "         '\\nEach separate color of region indicates extracted region'\n",
    "         % (n_regions_extracted, 20))\n",
    "plotting.plot_prob_atlas(regions_extracted_img, \n",
    "                         bg_img=mean_haxby, \n",
    "                         view_type='filled_contours',\n",
    "                         title=title)\n",
    "\n",
    "X = extractor.transform(func_file)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c9e45",
   "metadata": {},
   "source": [
    "So we have 1452 time points in the imaging data, and for each time point we have recordings of fMRI activity across 68 brain regions.\n",
    "\n",
    "## Create brain graph for GCN\n",
    "\n",
    "A key component of GCN is brain graph.\n",
    "Brain graph provides a network representation of brain organization by associating nodes to brain regions and defining edges via anatomical or functional connections.\n",
    "After generating time series, we will firstly use the nilearn function to geneate a correlation based functional connectome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "import nilearn.connectome\n",
    "\n",
    "# Estimating connectomes and save for pytorch to load\n",
    "corr_measure = nilearn.connectome.ConnectivityMeasure(kind=\"correlation\")\n",
    "conn = corr_measure.fit_transform([X])[0]\n",
    "\n",
    "title = 'Correlation between %d regions' % n_regions_extracted\n",
    "\n",
    "# First plot the matrix\n",
    "display = plotting.plot_matrix(conn, vmax=1, vmin=-1,\n",
    "                               colorbar=True, title=title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de68f87f",
   "metadata": {},
   "source": [
    "The next step is to construct the brain graph for GCN.\n",
    "\n",
    "__k-Nearest Neighbours(KNN) graph__ for the group average connectome will be built based on the connectivity-matrix.\n",
    "\n",
    "Each node is only connected to *k* conn = corr_measure.fit_transform([X])[0]\n",
    "other neighbouring nodes.\n",
    "For the purpose of demostration, we constrain the graph to from clusters with __8__ neighbouring nodes with the strongest connectivity.\n",
    "\n",
    "For more details you please check out __*src/graph_construction.py*__ script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "from graph_construction import make_group_graph\n",
    "\n",
    "# make a graph for the subject\n",
    "graph = make_group_graph([conn], self_loops=False, k=8, symmetric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82c8e5d",
   "metadata": {},
   "source": [
    "## Preparing the dataset for model training\n",
    "\n",
    "The trials for different object categories are scattered in the experiment. \n",
    "Firstly we will concatenated the volumes of the same category together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9afd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cancatenate the same type of trials\n",
    "concat_bold = {}\n",
    "for label in categories:\n",
    "    cur_label_index = y.index[y == label].tolist()\n",
    "    curr_bold_seg = X[cur_label_index]    \n",
    "    concat_bold[label] = curr_bold_seg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1696b8c",
   "metadata": {},
   "source": [
    "We split the data by the time window size that we wish to use to caputre the temporal dynamic.\n",
    "Different lengths for our input data can be selected. \n",
    "In this example we will continue with __*window_length = 1*__, which means each input file will have a length equal to just one Repetition Time (TR).\n",
    "The splitted timeseries are saved as individual files (in the format of `<category>_seg_<serialnumber>.npy`), \n",
    "the file names and the associated label are stored in the same directory,\n",
    "under a file named `label.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdefd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data by time window size and save to file\n",
    "window_length = 1\n",
    "dic_labels = {name: i for i, name in enumerate(categories)}\n",
    "\n",
    "# set output paths\n",
    "split_path = os.path.join(data_dir, 'haxby_split_win/')\n",
    "if not os.path.exists(split_path):\n",
    "    os.makedirs(split_path)\n",
    "out_file = os.path.join(split_path, '{}_{:04d}.npy')\n",
    "out_csv = os.path.join(split_path, 'labels.csv')\n",
    "\n",
    "label_df = pd.DataFrame(columns=['label', 'filename'])\n",
    "for label, ts_data in concat_bold.items():\n",
    "    ts_duration = len(ts_data)\n",
    "    ts_filename = f\"{label}_seg\"\n",
    "    valid_label = dic_labels[label]\n",
    "\n",
    "    # Split the timeseries\n",
    "    rem = ts_duration % window_length\n",
    "    n_splits = int(np.floor(ts_duration / window_length))\n",
    "\n",
    "    ts_data = ts_data[:(ts_duration - rem), :]   \n",
    "\n",
    "    for j, split_ts in enumerate(np.split(ts_data, n_splits)):\n",
    "        ts_output_file_name = out_file.format(ts_filename, j)\n",
    "\n",
    "        split_ts = np.swapaxes(split_ts, 0, 1)\n",
    "        np.save(ts_output_file_name, split_ts)\n",
    "\n",
    "        curr_label = {'label': valid_label, 'filename': os.path.basename(ts_output_file_name)}\n",
    "        label_df = label_df.append(curr_label, ignore_index=True)\n",
    "        \n",
    "label_df.to_csv(out_csv, index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb06af5",
   "metadata": {},
   "source": [
    "Now we use a customised `pytorch` dataset generator class `TimeWindowsDataset` to split the data into training, \n",
    "validation, and testing sets for model selection.\n",
    "\n",
    "```{admonition} Model selection\n",
    ":class: tip\n",
    "For further details of model selection, please check out the material from [this tutorial](https://github.com/neurodatascience/main-2021-ml-parts-1-2).\n",
    "```\n",
    "\n",
    "The dataset generator defaults isolates 20% of the data as the validation set, and 10% as testing set.\n",
    "For more details of customising a dataset, please see `src/gcn_windows_dataset.py` and the \n",
    "official [`pytorch` documentation](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eccfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "from gcn_windows_dataset import TimeWindowsDataset\n",
    "\n",
    "random_seed = 0\n",
    "\n",
    "train_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"train\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "valid_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"valid\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "test_dataset = TimeWindowsDataset(\n",
    "    data_dir=split_path, \n",
    "    partition=\"test\", \n",
    "    random_seed=random_seed, \n",
    "    pin_memory=True, \n",
    "    normalize=True,\n",
    "    shuffle=True)\n",
    "\n",
    "print(\"train dataset: {}\".format(train_dataset))\n",
    "print(\"valid dataset: {}\".format(valid_dataset))\n",
    "print(\"test dataset: {}\".format(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114daf5",
   "metadata": {},
   "source": [
    "Once the datasets are created, we can use the pytorch [data loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders) to iterate through the data during the model selection process.\n",
    "The __batch size__ defines the number of samples that will be propagated through the neural network.\n",
    "We are separating the dataset into 16 time windows per batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6eb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "torch.manual_seed(random_seed)\n",
    "train_generator = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_generator = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_generator = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_generator))\n",
    "print(f\"Feature batch shape: {train_features.size()}; mean {torch.mean(train_features)}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}; mean {torch.mean(torch.Tensor.float(train_labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11187324",
   "metadata": {},
   "source": [
    "## Generating a GCN model \n",
    "\n",
    "We have created a GCN of the following property:\n",
    "- __3__ graph convolutional layers\n",
    "- __32 graph filters__  at each layer\n",
    "- followed by a __global average pooling__ layer\n",
    "- __2 fully connected__ layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcn_model import GCN\n",
    "\n",
    "gcn = GCN(graph.edge_index, \n",
    "          graph.edge_attr, \n",
    "          n_roi=X.shape[1],\n",
    "          batch_size=batch_size,\n",
    "          n_timepoints=window_length, \n",
    "          n_classes=len(categories))\n",
    "gcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1df47",
   "metadata": {},
   "source": [
    "## Train and evaluating the model\n",
    "\n",
    "We will use a procedure called backpropagation to train the model.\n",
    "When we training the model with the first batch of data, the accuarcy and loss will be pretty poor.\n",
    "Backpropagation is an algorithm to update the model based on the rate of loss. \n",
    "Iterating through each batch, the model will be updated and reduce the loss.\n",
    "\n",
    "Function `training_loop` performs backpropagation through pytorch. \n",
    "One can use their own choice of optimizer for backpropagation and estimator for loss.\n",
    "\n",
    "After one round of training, we use the validation dataset to calculate the average accuracy and loss with function `valid_test_loop`. \n",
    "These metrics will serve as the reference for model performance of this round of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae177c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)    \n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss, current = loss.item(), batch * dataloader.batch_size\n",
    "\n",
    "        correct = (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        correct /= X.shape[0]\n",
    "        if (batch % 10 == 0) or (current == size):\n",
    "            print(f\"#{batch:>5};\\ttrain_loss: {loss:>0.3f};\\ttrain_accuracy:{(100*correct):>5.1f}%\\t\\t[{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        \n",
    "def valid_test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    loss /= size\n",
    "    correct /= size\n",
    "\n",
    "    return loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b7e03",
   "metadata": {},
   "source": [
    "This whole procedure described above is called an __epoch__.\n",
    "We will repeat the process for 60 epochs.\n",
    "Here the choice of loss function is `CrossEntropyLoss` and the optimizer to update the model is `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afbd8a",
   "metadata": {
    "tags": [
     "hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-4, weight_decay=5e-4)\n",
    "\n",
    "epochs = 60\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}/{epochs}\\n-------------------------------\")\n",
    "    train_loop(train_generator, gcn, loss_fn, optimizer)\n",
    "    loss, correct = valid_test_loop(valid_generator, gcn, loss_fn)\n",
    "    print(f\"Valid metrics:\\n\\t avg_loss: {loss:>8f};\\t avg_accuracy: {(100*correct):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9b088",
   "metadata": {},
   "source": [
    "After training the model for 60 epochs, we use the untouched test data to evaluate the model and conclude the results of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "loss, correct = valid_test_loop(test_generator, gcn, loss_fn)\n",
    "print(f\"Test metrics:\\n\\t avg_loss: {loss:>f};\\t avg_accuracy: {(100*correct):>0.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be0511e",
   "metadata": {},
   "source": [
    "The performance is not greate. How would you improve it?\n",
    "\n",
    "## Exercises\n",
    " * Try out different time window size, batch size for the dataset,\n",
    " * Try different brain graph construction methods.\n",
    " * Try use different loss function or optimizer function.\n",
    " * **Hard**: Treat the parameters you changed, such as time window size and batch size, as parameters of part of the model training.\n",
    "\n",
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   33,
   50,
   54,
   58,
   78,
   103,
   119,
   149,
   159,
   175,
   187,
   194,
   202,
   213,
   222,
   256,
   270,
   303,
   309,
   322,
   332,
   342,
   357,
   393,
   399,
   410,
   414,
   418
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}