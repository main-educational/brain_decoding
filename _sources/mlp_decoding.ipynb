{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ecc5d8",
   "metadata": {},
   "source": [
    "# Brain decoding with MLP\n",
    "\n",
    "## Multilayer Perceptron\n",
    "```{figure} mlp_decoding/multilayer-perceptron.png\n",
    "---\n",
    "width: 800px\n",
    "name: multilayer-perceptron-fig\n",
    "---\n",
    "A multilayer perceptron with 25 units on the input layer, a single hidden layer with 17 units, and an output layer with 9 units. Figure generated with the [NN-SVG](http://alexlenail.me/NN-SVG/index.html) tool by [Alexander Lenail]. The figure is shared under a [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "```\n",
    "We are going to train a Multilayer Perceptron (MLP) classifier for brain decoding on the Haxby dataset. MLPs are one of the most basic architecture of artificial neural networks. MLPs consist of input and output layers as well as hidden layers that transform the input to the usable data for the output layer. Like other machine learning models for supervised learning, a MLP initially goes through a training phase. During this supervised phase, the network is taught what to look for and what is the desired output.\n",
    "In this tutorial, we are going to train the simplest MLP architecture featuring one input layer, one output layer and just one hidden layer.\n",
    "\n",
    "## Getting the data\n",
    "We are going to download the dataset from Haxby and colleagues (2001) {cite:p}`Haxby2001-vt`. You can check section {ref}`haxby-dataset` for more details on that dataset. Here we are going to quickly download it, and prepare it for machine learning applications with a set of predictive variable, the brain time series `X`, and a dependent variable, the annotation on cognition `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2184c6",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "from nilearn import datasets\n",
    "# We are fetching the data for subject 4\n",
    "data_dir = os.path.join('..', 'data')\n",
    "sub_no = 4\n",
    "haxby_dataset = datasets.fetch_haxby(subjects=[sub_no], fetch_stimuli=True, data_dir=data_dir)\n",
    "func_file = haxby_dataset.func[0]\n",
    "\n",
    "# mask the data\n",
    "from nilearn.input_data import NiftiMasker\n",
    "mask_filename = haxby_dataset.mask_vt[0]\n",
    "masker = NiftiMasker(mask_img=mask_filename, standardize=True, detrend=True)\n",
    "X = masker.fit_transform(func_file)\n",
    "\n",
    "# cognitive annotations\n",
    "import pandas as pd\n",
    "behavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\n",
    "y = behavioral['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b15b9",
   "metadata": {},
   "source": [
    "Let's check the size of `X` and `y`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658f3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = y.unique()\n",
    "print(categories)\n",
    "print(y.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30239ae9",
   "metadata": {},
   "source": [
    "So we have 1452 time points, with one cognitive annotations each, and for each time point we have recordings of fMRI activity across 675 voxels. We can also see that the cognitive annotations span 9 different categories.\n",
    "\n",
    "We are going to use Keras for training the MLP, and we are going to convert the string categories into a one-hot encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874d5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "y_onehot = enc.fit_transform(np.array(y).reshape(-1, 1))\n",
    "# turn the sparse matrix into a pandas dataframe\n",
    "y = pd.DataFrame(y_onehot.toarray())\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1861d3",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "We are going to start by splitting our dataset between train and test. We will keep 20% of the time points as test, and then set up a 10 fold cross validation for training/validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60892b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99daf5b1",
   "metadata": {},
   "source": [
    "Now we can build a MLP using Tensorflow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# number of unique conditions that we have\n",
    "model_mlp = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model_mlp.add(Dense(50 , input_dim = 675, kernel_initializer=\"uniform\", activation = 'relu'))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model_mlp.add(Dense(30, kernel_initializer=\"uniform\", activation = 'relu'))\n",
    "\n",
    "# Using softmax at the end, length of categories shows the number of labels we have\n",
    "model_mlp.add(Dense(len(categories), activation = 'softmax'))\n",
    "\n",
    "model_mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e2992",
   "metadata": {},
   "source": [
    "Time to train that model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2400c7",
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model_mlp.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "# Fitting the model on the Training set\n",
    "history = model_mlp.fit(X_train, y_train, batch_size = 10,\n",
    "                             epochs = 10, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f209cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "import visualization\n",
    "plot_history = visualization.classifier_history (history, 'MLP ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b17644",
   "metadata": {},
   "source": [
    "## Assessing performance\n",
    "Let's check the accuracy of the prediction on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2f651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the predictions and evaluating the model\n",
    "from sklearn.metrics import classification_report\n",
    "y_train_pred = model_mlp.predict(X_train)\n",
    "print(classification_report(y_train.values.argmax(axis = 1), y_train_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6208d76d",
   "metadata": {},
   "source": [
    "This is dangerously high. Let's check on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f85c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model_mlp.predict(X_test)\n",
    "print(classification_report(y_test.values.argmax(axis = 1), y_test_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac840e3",
   "metadata": {},
   "source": [
    "We can have a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d8aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sys.path.append('../src')\n",
    "import visualization\n",
    "cm_svm = confusion_matrix(y_test.values.argmax(axis = 1), y_test_pred.argmax(axis=1))\n",
    "model_conf_matrix = cm_svm.astype('float') / cm_svm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "visualization.conf_matrix(model_conf_matrix,\n",
    "                          categories,\n",
    "                          title='MLP decoding results on Haxby')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aab4e8",
   "metadata": {},
   "source": [
    "```{warning}\n",
    "Unfortunately we don't have a simple way to visualize the important features like we did with the linear SVM! You can check this fantastic [distill article](https://distill.pub/2017/feature-visualization/) to learn more about feature visualization in artificial neural networks.\n",
    "```\n",
    "\n",
    "## Exercises\n",
    " * What is the most difficult category to decode? Why?\n",
    " * The model seemed to overfit. Try adding a `Dropout` layer to regularize the model. You can read about dropout in keras in this [blog post](https://towardsdatascience.com/machine-learning-part-20-dropout-keras-layers-explained-8c9f6dc4c9ab).\n",
    " * Try to add layers or hidden units, and observe the impact on overfitting and training time."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   31,
   54,
   57,
   62,
   67,
   76,
   80,
   83,
   86,
   103,
   106,
   114,
   119,
   123,
   128,
   130,
   133,
   136,
   149
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}